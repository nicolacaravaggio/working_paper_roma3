{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web-scraping of *Departmental Working Papers of Economics, Roma Tre University*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reqruired libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from urllib.request import Request, urlopen\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "from IPython.core.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from warnings import warn\n",
    "warn(\"Warning Simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve data from IDEAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the number of pages\n",
    "\n",
    "url = 'https://ideas.repec.org/s/rtr/wpaper.html'\n",
    "\n",
    "response = get(url)\n",
    "\n",
    "html_soup = bs(response.text, 'html.parser')\n",
    "\n",
    "containers = html_soup.find_all('a', class_ = 'page-link')\n",
    "\n",
    "pages = []\n",
    "\n",
    "for container in containers:\n",
    "    pages.append(container.text)\n",
    "    \n",
    "x = [s for s in pages if s != \"«\" and s != \"»\" and s != \"1\"]\n",
    "\n",
    "x = list(dict.fromkeys(x))\n",
    "\n",
    "y = ['']\n",
    "\n",
    "pages = y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve paper name, authors, and year of publication\n",
    "\n",
    "year_list = []\n",
    "title_list = []\n",
    "author_list = []\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    sleep(randint(1,3))\n",
    "    \n",
    "    url = 'https://ideas.repec.org/s/rtr/wpaper' + page + '.html'\n",
    "    \n",
    "    response = get(url)\n",
    "    \n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    \n",
    "    container = soup.select_one(\"#content\")\n",
    "    \n",
    "    year_list.extend([int(h.text) for h in container.find_all('h3')])\n",
    "    \n",
    "    for panel in container.select(\"div.panel-body\"):\n",
    "        \n",
    "        title_list.append([x.text for x in panel.find_all('a')])\n",
    "    \n",
    "        author_list.append([x.next_sibling.strip() for x in panel.find_all('i')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique dataframe\n",
    "\n",
    "df_title = pd.DataFrame(title_list) \n",
    "df_author = pd.DataFrame(author_list)\n",
    "df_year = pd.DataFrame(year_list)\n",
    "df_year = df_year.rename(columns = {0: 'year'})\n",
    "\n",
    "df_year_author = pd.concat([df_year, df_author.reindex(df_year.index)], axis = 1)\n",
    "\n",
    "dup_year_1 = df_year_author[df_year_author['year'].duplicated(keep = 'last')]\n",
    "dup_year_1.dropna(axis = 1, how = 'all', inplace = True)\n",
    "dup_year_2 = df_year_author[df_year_author['year'].duplicated(keep = 'first')]\n",
    "dup_year_2.dropna(axis = 1, how = 'all', inplace = True)\n",
    "\n",
    "dup_year_conc = pd.merge(dup_year_1, dup_year_2, how = 'outer', on = ['year'])\n",
    "dup_year_conc_y = dup_year_conc['year']\n",
    "dup_year_conc = dup_year_conc.loc[:, dup_year_conc.columns != 'year']\n",
    "dup_year_conc.columns = range(dup_year_conc.shape[1])\n",
    "\n",
    "dup_year = pd.concat([dup_year_conc_y,dup_year_conc], axis = 1)\n",
    "df_year_author_no_dup = df_year_author.drop_duplicates(subset = 'year', keep = False) \n",
    "\n",
    "df_year_author = df_year_author_no_dup.append(dup_year)\n",
    "df_year_author = df_year_author.sort_values(['year'], ascending = [False])\n",
    "df_year_author = df_year_author.add_prefix('col_')\n",
    "df_year_author = df_year_author.rename(columns = {'col_year': 'year_pub'})\n",
    "df_year_author = df_year_author.reset_index()\n",
    "\n",
    "df_year_title = pd.concat([df_year, df_title.reindex(df_year.index)], axis = 1)\n",
    "\n",
    "dup_year_1 = df_year_title[df_year_title['year'].duplicated(keep = 'last')]\n",
    "dup_year_1.dropna(axis = 1, how = 'all', inplace = True)\n",
    "dup_year_2 = df_year_title[df_year_title['year'].duplicated(keep = 'first')]\n",
    "dup_year_2.dropna(axis = 1, how = 'all', inplace = True)\n",
    "\n",
    "dup_year_conc = pd.merge(dup_year_1, dup_year_2, how = 'outer', on = ['year'])\n",
    "dup_year_conc_y = dup_year_conc['year']\n",
    "dup_year_conc = dup_year_conc.loc[:, dup_year_conc.columns != 'year']\n",
    "dup_year_conc.columns = range(dup_year_conc.shape[1])\n",
    "\n",
    "dup_year = pd.concat([dup_year_conc_y,dup_year_conc], axis = 1)\n",
    "df_year_title_no_dup = df_year_title.drop_duplicates(subset = 'year', keep = False) \n",
    "\n",
    "df_year_title = df_year_title_no_dup.append(dup_year)\n",
    "df_year_title = df_year_title.sort_values(['year'], ascending = [False])\n",
    "df_year_title = df_year_title.add_prefix('col_')\n",
    "df_year_title = df_year_title.rename(columns = {'col_year': 'year_pub'})\n",
    "df_year_title = df_year_title.reset_index()\n",
    "\n",
    "df_author = pd.wide_to_long(df_year_author, stubnames = 'col_', i = 'year_pub', j = 'ord_pub')\n",
    "df_author = df_author.rename(columns = {'col_': 'author'})\n",
    "df_author = df_author.drop(['index'], axis = 1)\n",
    "\n",
    "df_title = pd.wide_to_long(df_year_title, stubnames = 'col_', i = 'year_pub', j = 'ord_pub')\n",
    "df_title = df_title.rename(columns = {'col_': 'title'})\n",
    "df_title = df_title.drop(['index'], axis = 1)\n",
    "\n",
    "df = pd.concat([df_title, df_author.reindex(df_title.index)], axis = 1)\n",
    "df = df.sort_values(['year_pub'], ascending = [False])\n",
    "df = df.reset_index()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data frames by years\n",
    "\n",
    "grouped_years = df.groupby(df.year_pub)\n",
    "\n",
    "for year in year_list:\n",
    "    globals()['df_' + str(year)] = grouped_years.get_group(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic IP and Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following commands have been retrieved from the following sources: \n",
    "# - https://codelike.pro/create-a-crawler-with-rotating-ip-proxy-in-python/\n",
    "# - https://stackoverflow.com/questions/38785877/spoofing-ip-address-when-web-scraping-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#49: 128.199.251.249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a list of proxies\n",
    "ua = UserAgent() # it will generate a random user agent\n",
    "proxies = [] # it will contain proxies [ip, port]\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "  # Retrieve latest proxies\n",
    "  proxies_req = Request('https://www.sslproxies.org/')\n",
    "  proxies_req.add_header('User-Agent', ua.random)\n",
    "  proxies_doc = urlopen(proxies_req).read().decode('utf8')\n",
    "\n",
    "  soup = bs(proxies_doc, 'html.parser')\n",
    "  proxies_table = soup.find(id='proxylisttable')\n",
    "\n",
    "  # Save proxies in the array\n",
    "  for row in proxies_table.tbody.find_all('tr'):\n",
    "    proxies.append({\n",
    "      'ip':   row.find_all('td')[0].string,\n",
    "      'port': row.find_all('td')[1].string\n",
    "    })\n",
    "\n",
    "  # Choose a random proxy\n",
    "  proxy_index = random_proxy()\n",
    "  proxy = proxies[proxy_index]\n",
    "\n",
    "  # Perform 50 requests\n",
    "  for n in range(1, 50):\n",
    "    req = Request('http://icanhazip.com')\n",
    "    req.set_proxy(proxy['ip'] + ':' + proxy['port'], 'http')\n",
    "\n",
    "    # Every 10 requests, generate a new proxy\n",
    "    if n % 10 == 0:\n",
    "      proxy_index = random_proxy()\n",
    "      proxy = proxies[proxy_index]\n",
    "\n",
    "    # Make the call\n",
    "    try:\n",
    "      my_ip = urlopen(req).read().decode('utf8')\n",
    "      print('#' + str(n) + ': ' + my_ip)\n",
    "      clear_output(wait = True)\n",
    "    except: # If error, delete this proxy and find another one\n",
    "      del proxies[proxy_index]\n",
    "      print('Proxy ' + proxy['ip'] + ':' + proxy['port'] + ' deleted.')\n",
    "      proxy_index = random_proxy()\n",
    "      proxy = proxies[proxy_index]\n",
    "\n",
    "# Retrieve a random index proxy\n",
    "def random_proxy():\n",
    "  return random.randint(0, len(proxies) - 1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following command has been retrieved from the following source:\n",
    "# - https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "# List of user agents\n",
    "user_agent_list = (\n",
    "    #Chrome\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    #Firefox\n",
    "    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract citations from Google Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:\n",
    "# In order to avoid the bann from Google, you can use random user agent and proxy\n",
    "# Moreover, you can also change your IP (e.g. by using Hide.me: https://hide.me/en/)\n",
    "# A wise choice is to make the process 'slow'\n",
    "\n",
    "start = timer()\n",
    "\n",
    "title_list_gs = []\n",
    "citations_list_gs = []\n",
    "link_list = []\n",
    "\n",
    "for year in year_list:\n",
    "    \n",
    "    globals()['df_' + str(year)]['titles_authors'] = globals()['df_' + str(year)]['title'] + ' ' + globals()['df_' + str(year)]['author']\n",
    "    titles_authors = globals()['df_' + str(year)]['titles_authors'].to_list()\n",
    "\n",
    "    # Print results\n",
    "    print('\\n', year)    \n",
    "    \n",
    "    title_list_gs2 = []\n",
    "    citations_list_gs2 = []\n",
    "    link_list2 = []\n",
    "        \n",
    "    with requests.Session() as s:\n",
    "        \n",
    "        for title in titles_authors: \n",
    "            \n",
    "            sleep(randint(30, 60)) # Run a query every x-y seconds\n",
    "            \n",
    "            user_agent = random.choice(user_agent_list)\n",
    "            headers= {'User-Agent': user_agent, \"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "            #proxy = random.choice(proxies) # Comment out to use random proxies\n",
    "                \n",
    "            url = 'https://scholar.google.com/scholar?q=' + title + '&ie=UTF-8&oe=UTF-8&hl=en&btnG=Search'\n",
    "\n",
    "            r = s.get(url, headers = headers)\n",
    "            #r = s.get(url, headers = headers, proxies = proxy) # Comment out to use random proxies\n",
    "            soup = bs(r.content, 'html.parser')\n",
    "            \n",
    "            # Retrieve title\n",
    "            title_gs = soup.select_one('h3.gs_rt a').text if soup.select_one('h3.gs_rt a') is not None else 'No title'\n",
    "            title_list_gs.append(title_gs)\n",
    "            title_list_gs2.append(title_gs)\n",
    "            \n",
    "            # Retrieve number of citations\n",
    "            citations_gs = soup.select_one('a:contains(\"Cited by\")').text if soup.select_one('a:contains(\"Cited by\")') is not None else 'No citation count'\n",
    "            citations_list_gs.append(citations_gs)\n",
    "            citations_list_gs2.append(citations_gs)\n",
    "\n",
    "            # Retrieve link\n",
    "            link_gs = soup.select_one('h3.gs_rt a')['href'] if title != 'No title' else 'No link'\n",
    "            link_list.append(link_gs)\n",
    "            link_list2.append(link_gs)\n",
    "            \n",
    "            print('Title:', title_gs, '; Citations:', citations_gs) \n",
    "            \n",
    "    globals()['df_' + str(year)]['citations'] = np.array(citations_list_gs2) \n",
    "    globals()['df_' + str(year)]['citations'] = globals()['df_' + str(year)]['citations'].str.extract('(\\d+)').astype(float) \n",
    "    \n",
    "    sleep(600) # Stop for 10 minutes\n",
    "    \n",
    "# Required time\n",
    "end = timer()\n",
    "print('\\n Retrieve data from from Google Scholar')\n",
    "print('- required time:') \n",
    "print(timedelta(seconds = end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all sub data frames\n",
    "\n",
    "df_names = []\n",
    "\n",
    "for year in year_list:\n",
    "    df_names.append('df_' + str(year))\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for df_ in df_names:\n",
    "    df_list.append(locals()[df_])\n",
    "    \n",
    "df_all = pd.concat(df_list)\n",
    "\n",
    "# Eliminate outliers: when the number of citations is too high, subtitute with NaN\n",
    "def erasecit(x):\n",
    "    if x >= 100:\n",
    "        return None\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df_all['citations'] = df_all['citations'].apply(erasecit)\n",
    "    \n",
    "df_all.to_csv('C:/Users/Nicola Caravaggio/OneDrive/Desktop/rm3_web_scraping.csv', index = False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange data\n",
    "df_count = df_all.groupby(['year_pub']).count()\n",
    "df_count['year_pub'] = df_count['year_pub'].astype(str)\n",
    "\n",
    "# Plot\n",
    "sns.set(style = \"white\", rc = {\"lines.linewidth\": 1})\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "ax1.set_title('Working Papers Roma Tre University', fontsize=16)\n",
    "\n",
    "# Bar plot creation\n",
    "ax1 = sns.barplot(x = 'year_pub', \n",
    "                  y = 'title', \n",
    "                  data = df_count, \n",
    "                  palette = 'summer')\n",
    "ax1.set_ylim([0, 35]) \n",
    "ax1.set_xlabel('Year', \n",
    "               fontsize = 14)\n",
    "ax1.set_ylabel('Publications', \n",
    "               fontsize = 14)\n",
    "ax1.tick_params(axis = 'y')\n",
    "\n",
    "# Mirror second axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Line plot creation\n",
    "ax2 = sns.lineplot(x = 'year_pub', \n",
    "                   y = 'citations', \n",
    "                   data = df_count, \n",
    "                   sort = False, \n",
    "                   color = 'red')\n",
    "ax2.set_ylim([0, 35])\n",
    "ax2.set_ylabel('Citations', \n",
    "               fontsize = 14)\n",
    "ax2.tick_params(axis = 'y')\n",
    "\n",
    "ax1.tick_params('x', labelrotation = 45)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
